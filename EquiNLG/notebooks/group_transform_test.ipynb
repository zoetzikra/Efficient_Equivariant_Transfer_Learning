{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e083902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19c27523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_group_generator(vocab_size, group_size, eq_indices):\n",
    "    \"\"\"\n",
    "    :param vocab_size: size of the vocab\n",
    "    :param group_size: size of the group\n",
    "    :param eq_indices: set of  list of indices of the tokens for the equivariant words in the vocab; each list of indices of the same size as group size\n",
    "    :return: a group generator of the required cyclic group consisting of all the equivariant word indices\n",
    "    \"\"\"\n",
    "    g = {i: i for i in range(vocab_size)}  # generator initialized as id\n",
    "\n",
    "    for i in range(group_size):\n",
    "        next_group_element = (i + 1) % group_size\n",
    "        # g[eq_indices[i]] = eq_indices[next_group_element]\n",
    "        for j in range(len(eq_indices)):\n",
    "            g[eq_indices[j][i]] = eq_indices[j][next_group_element]\n",
    "\n",
    "    g['___size___'] = group_size  # add length of the group as a value\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71da2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_group(g, vocab_size, group_size):\n",
    "    \"\"\"\n",
    "    :param g: cyclic group generator\n",
    "    :param group_size: size of the group\n",
    "    :return: return a list of elements of a cyclic group\n",
    "    \"\"\"\n",
    "    # add id to the group G\n",
    "    G = [{i: i for i in range(vocab_size)}]\n",
    "\n",
    "    for i in range(group_size - 1):\n",
    "        # apply the generator repeatedly to obtain the entire group\n",
    "        curr_g = G[-1]\n",
    "        next_g = {i: g[curr_g[i]] for i in range(vocab_size)}\n",
    "        G.append(next_g)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d04b94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_transform_data(data, G, device):\n",
    "    '''\n",
    "    :param data: any tensor data of input on which group is applied\n",
    "    :param G: set of group elements\n",
    "    :return: list of transformed data for equituning\n",
    "    '''\n",
    "    # print(\"Debugging function: g_transform_data\")\n",
    "    # print(\"  Group Elements:\", G)\n",
    "\n",
    "    data_shape = data.size()\n",
    "    untransformed_data = data.view(-1)\n",
    "    transformed_data = [untransformed_data]\n",
    "\n",
    "    for i in range(len(G)-1):\n",
    "        curr_g = G[i+1]\n",
    "        current_data = torch.tensor(itemgetter(*(untransformed_data.tolist()))(curr_g), device=device)\n",
    "        transformed_data.append(current_data)\n",
    "        # print(f\"  After applying group element {i}: {current_data}\")\n",
    "\n",
    "    transformed_data = torch.stack(transformed_data).view(len(G), data_shape[0], data_shape[1])\n",
    "    transformed_data.to(device)\n",
    "\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a575361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_inv_transform_prob_data(data_list, G):\n",
    "    '''\n",
    "    Note: Group actions are on batch_size x |V|, instead of batch_size x 1\n",
    "    :param data: any tensor data\n",
    "    :param g: group generator\n",
    "    :return: list of transformed data for equituning\n",
    "    '''\n",
    "    # print(\"Debugging function: g_inv_transform_prob_data\")\n",
    "    output_data_list = data_list.clone()  # dim [group_size, batch_size, num_tokens, |V|]\n",
    "    g_indices = []\n",
    "    for g in G:\n",
    "        print(f\"g: {g}\")\n",
    "        g_index = [g[i] for i in range(len(g))]\n",
    "        print(f\"g_index: {g_index}\")\n",
    "        g_indices.append(g_index)\n",
    "        print()\n",
    "\n",
    "    # print(\"  Initial data list for inverse transformation:\")\n",
    "    # print(data_list)\n",
    "\n",
    "    for i in range(len(data_list)):  # iterate over group size\n",
    "        output_data_list[i, :, :, g_indices[i]] = output_data_list[i, :, :, :].clone()\n",
    "\n",
    "    # print(\"  Final data list after inverse transformation:\")\n",
    "    # print(output_data_list)\n",
    "    return output_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9122a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demographic: MAN vs WOMAN\n",
    "# E: [[man, woman], [he, she]]\n",
    "vocab_size = 7\n",
    "group_size = 2\n",
    "eq_indices = [[0, 1], [2, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6927a666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1, 1: 0, 2: 3, 3: 2, 4: 4, 5: 5, 6: 6, '___size___': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_generator = cyclic_group_generator(vocab_size, group_size, eq_indices)\n",
    "group_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bb7b53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6},\n",
       " {0: 1, 1: 0, 2: 3, 3: 2, 4: 4, 5: 5, 6: 6}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = cyclic_group(group_generator, vocab_size, group_size)\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd23a995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data: two sentences\n",
    "# sent1: man is smart -> [0, 4, 5]\n",
    "# sent2: he is good -> [2, 4, 6]\n",
    "\n",
    "data = torch.tensor([[0, 4, 5], [2, 4, 6]])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5a50045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 4, 5],\n",
       "         [2, 4, 6]],\n",
       "\n",
       "        [[1, 4, 5],\n",
       "         [3, 4, 6]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data = g_transform_data(data, G, device='cpu')\n",
    "transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a0feae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 0, 0, 0, 0, 0, 0],\n",
       "          [4, 4, 4, 4, 4, 4, 4],\n",
       "          [5, 5, 5, 5, 5, 5, 5]],\n",
       "\n",
       "         [[2, 2, 2, 2, 2, 2, 2],\n",
       "          [4, 4, 4, 4, 4, 4, 4],\n",
       "          [6, 6, 6, 6, 6, 6, 6]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1],\n",
       "          [4, 4, 4, 4, 4, 4, 4],\n",
       "          [5, 5, 5, 5, 5, 5, 5]],\n",
       "\n",
       "         [[3, 3, 3, 3, 3, 3, 3],\n",
       "          [4, 4, 4, 4, 4, 4, 4],\n",
       "          [6, 6, 6, 6, 6, 6, 6]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulated_model_output = transformed_data.clone().unsqueeze(-1).expand(-1, -1, -1, vocab_size)\n",
    "simulated_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e302531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 7])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulated_model_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b98b6856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 4, 5],\n",
       "        [2, 4, 6],\n",
       "        [1, 4, 5],\n",
       "        [3, 4, 6]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulated_model_output[:, :, :, 0].view(-1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99b7685a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6}\n",
      "g_index: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "g: {0: 1, 1: 0, 2: 3, 3: 2, 4: 4, 5: 5, 6: 6}\n",
      "g_index: [1, 0, 3, 2, 4, 5, 6]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 0, 0, 0, 0, 0, 0],\n",
       "          [4, 4, 4, 4, 4, 4, 4],\n",
       "          [5, 5, 5, 5, 5, 5, 5]],\n",
       "\n",
       "         [[2, 2, 2, 2, 2, 2, 2],\n",
       "          [4, 4, 4, 4, 4, 4, 4],\n",
       "          [6, 6, 6, 6, 6, 6, 6]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1],\n",
       "          [4, 4, 4, 4, 4, 4, 4],\n",
       "          [5, 5, 5, 5, 5, 5, 5]],\n",
       "\n",
       "         [[3, 3, 3, 3, 3, 3, 3],\n",
       "          [4, 4, 4, 4, 4, 4, 4],\n",
       "          [6, 6, 6, 6, 6, 6, 6]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_output = g_inv_transform_prob_data(simulated_model_output, G)\n",
    "inverted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f24f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_inv_transform_prob_data(data_list, G):\n",
    "    output_data_list = data_list.clone()\n",
    "    \n",
    "    return output_data_list[:, :, :, 0].view(-1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "268279ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_inv_transform_prob_data(data_list, G):\n",
    "    output_data_list = data_list.clone()  # Clone to avoid modifying the original data\n",
    "\n",
    "    # Revert transformations for each group element\n",
    "    for idx, g in enumerate(G):\n",
    "        for i in range(data_list.shape[1]):  # Iterating over each sequence element\n",
    "            for j in range(data_list.shape[2]):  # Iterating over each feature/vocabulary element\n",
    "                output_data_list[idx, i, j, :] = data_list[idx, i, g[j], :]\n",
    "\n",
    "    return output_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b06e6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_inv_transform_prob_data(data_list, G):\n",
    "    # Dimensions of data_list: [group_size, num_sentences, sentence_length, vocab_size]\n",
    "    # Prepare the output tensor with the same dimensions\n",
    "    #output_data_list = data_list\n",
    "    \n",
    "    # For each group transformation (there are as many transformations as the size of G)\n",
    "    for idx, g in enumerate(G):\n",
    "        # Invert the current group mapping\n",
    "        inv_g = {v: k for k, v in g.items()}\n",
    "        \n",
    "        # Apply the inverted group mapping to reorder each word in each sentence\n",
    "        for sent_idx in range(data_list.shape[1]):\n",
    "            # Get the original positions for all word indices in the sentence\n",
    "            original_indices = [inv_g[i] if i in inv_g else i for i in range(data_list.shape[2])]\n",
    "            \n",
    "            # Apply the inverse transformation correctly\n",
    "            for word_idx, orig_idx in enumerate(original_indices):\n",
    "                output_data_list[idx, sent_idx, word_idx, :] = data_list[idx, sent_idx, orig_idx, :]\n",
    "\n",
    "    return output_data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca9ea1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6},\n",
       " {0: 1, 1: 0, 2: 3, 3: 2, 4: 4, 5: 5, 6: 6}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7e9e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_inv_transform_prob_data(data_list, G):\n",
    "    '''\n",
    "    Note: Group actions are on batch_size x |V|, instead of batch_size x 1\n",
    "    :param data: any tensor data\n",
    "    :param g: group generator\n",
    "    :return: list of transformed data for equituning\n",
    "    '''\n",
    "    # print(\"Debugging function: g_inv_transform_prob_data\")\n",
    "    output_data_list = data_list.clone()  # dim [group_size, batch_size, num_tokens, |V|]\n",
    "    g_indices = []\n",
    "    for g in G:\n",
    "        # print(f\"g: {g}\")\n",
    "        g_index = [g[i] for i in range(len(g))]\n",
    "        # print(f\"g_index: {g_index}\")\n",
    "        g_indices.append(g_index)\n",
    "        # print()\n",
    "\n",
    "    # print(\"  Initial data list for inverse transformation:\")\n",
    "    # print(data_list)\n",
    "    for i in range(len(data_list)):  # iterate over group size\n",
    "        print(f\"i: {i}\")\n",
    "        print(f\"  g_indices[i]: {g_indices[i]}\")\n",
    "        print(f\"  output_data_list[i, :, :, g_indices[i]]: {output_data_list[i, :, :, g_indices[i]]}\")\n",
    "        print(f\"  output_data_list[i, :, :, :]: {output_data_list[i, :, :, :]}\")\n",
    "        output_data_list[i, :, :, g_indices[i]] = output_data_list[i, :, :, :].clone()\n",
    "\n",
    "    # print(\"  Final data list after inverse transformation:\")\n",
    "    # print(output_data_list)\n",
    "    return output_data_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lambda)",
   "language": "python",
   "name": "lambda_equitune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
